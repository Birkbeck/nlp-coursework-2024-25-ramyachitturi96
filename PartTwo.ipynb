{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be096ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b579b9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_clean_file(path):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # rename the ‘Labour (Co-op)’ value in ‘party’ column to ‘Labour’\n",
    "    df[\"party\"] = df[\"party\"].replace(\"Labour (Co-op)\", \"Labour\")\n",
    "    #print(df.shape)\n",
    "\n",
    "    # remove any rows where the value of the ‘party’ column is not one of the four most common party names, and remove the ‘Speaker’ value\n",
    "    df = df[df[\"party\"] != \"Speaker\"]\n",
    "    top4_parties = df[\"party\"].value_counts().index[:4]\n",
    "    df = df[df[\"party\"].isin(top4_parties)]\n",
    "    #print(df.shape)\n",
    "\n",
    "    # remove any rows where the value in the ‘speech_class’ column is not ‘Speech’\n",
    "    df = df[df[\"speech_class\"] != \"Speaker\"]\n",
    "    #print(df.shape)\n",
    "\n",
    "    #remove any rows where the text in the ‘speech’ column is less than 1000 characters long.\n",
    "    df = df[df[\"speech\"].str.len() >= 1000]\n",
    "    #print(df.shape)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_and_clean_file(\"p2-texts/hansard40000.csv\")\n",
    "df[\"party\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0bac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2a\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b8595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de4c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tfidfvectorizer_split_data(df, ngram: str):\n",
    "    X = df[\"speech\"]\n",
    "    y = df[\"party\"]\n",
    "    if ngram == \"tri-gram\":\n",
    "        vectorizer = TfidfVectorizer(stop_words = \"english\", max_features = 3000, ngram_range=(1,3))\n",
    "    else :\n",
    "        vectorizer = TfidfVectorizer(stop_words = \"english\", max_features = 3000)\n",
    "    X_vector = vectorizer.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_vector, y, test_size=0.2, stratify=y, random_state=26)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0231e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2b\n",
    "X_train, X_test, Y_train, Y_test = Tfidfvectorizer_split_data(df, \"default\")\n",
    "print(\"X Train set shape:\", X_train.shape)\n",
    "print(\"X Test set shape:\", X_test.shape)\n",
    "print(\"Y Train set shape:\", Y_train.shape)\n",
    "print(\"Y Test set shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0db6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e8ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Forest_and_SVM(X_train, X_test, y_train, y_test):\n",
    "    results = {}\n",
    "    #models\n",
    "    random_forest = RandomForestClassifier(n_estimators=300, random_state=26)\n",
    "    svm = SVC(kernel='linear', random_state=26)\n",
    "\n",
    "    #Training models\n",
    "    print(\"Training Random Forest model\")\n",
    "    random_forest.fit(X_train, y_train)\n",
    "    print(\"Training SVM model\")\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    #predictions\n",
    "    random_forest_pred = random_forest.predict(X_test)\n",
    "    svm_pred = svm.predict(X_test)\n",
    "\n",
    "    #macro-average f1 score\n",
    "    random_forest_f1 = f1_score(y_test, random_forest_pred, average='macro')\n",
    "    results[\"Random Forest\"] = random_forest_f1\n",
    "    svm_f1 = f1_score(y_test, svm_pred, average='macro')\n",
    "    results[\"SVM\"] = svm_f1\n",
    "\n",
    "    #classification report \n",
    "    random_forest_report = classification_report(y_test, random_forest_pred)\n",
    "    svm_report = classification_report(y_test, svm_pred)\n",
    "\n",
    "    print(\"Random Forest Model\")\n",
    "    print(f\"Macro-average f1 score: {random_forest_f1}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(random_forest_report)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"SVM Model\")\n",
    "    print(f\"Macro-average f1 score: {svm_f1}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(svm_report)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd26466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2c\n",
    "Random_Forest_and_SVM(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32639ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2d\n",
    "X_train, X_test, Y_train, Y_test = Tfidfvectorizer_split_data(df, \"tri-gram\")\n",
    "Random_Forest_and_SVM(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a4a60",
   "metadata": {},
   "source": [
    "Implement a new custom tokenizer and pass it to the tokenizer argument of Tfidfvectorizer. You can use this function in any way you like to try to achieve\n",
    "the best classification performance while keeping the number of features to nomore than 3000, and using the same three classifiers as above. Print the classification report for the best performing classifier using your tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "91d6162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7c08d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "\n",
    "    # Process text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        # Skip  punctuation, spaces, and numbers\n",
    "        if  token.is_punct or token.is_space or token.like_num:\n",
    "            continue\n",
    "            \n",
    "        # Filter by part-of-speech: nouns, verbs, adjectives, and adverbs\n",
    "        if token.pos_ in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]:\n",
    "            # Use lemma in lowercase\n",
    "            lemma = token.lemma_.lower().strip()\n",
    "            \n",
    "            # Additional filtering: require at least 3 characters and no special chars\n",
    "            if len(lemma) >= 3 and re.match((r'^[a-zA-Z]+$', lemma)):\n",
    "                tokens.append(lemma)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def Tfidfvectorizer_customtokeniser_split_data(df, ngram: str):\n",
    "    X = df[\"speech\"]\n",
    "    y = df[\"party\"]\n",
    "    if ngram == \"tri-gram\":\n",
    "        vectorizer = TfidfVectorizer(stop_words = \"english\", max_features = 3000, ngram_range=(1,3),tokenizer=custom_tokenizer,min_df=20,max_df=0.7)\n",
    "        X_vector = vectorizer.fit_transform(X)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_vector, y, test_size=0.2, stratify=y, random_state=26)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fa5113fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "match() missing 1 required positional argument: 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X_train, X_test, y_train, y_test = \u001b[43mTfidfvectorizer_customtokeniser_split_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtri-gram\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m Random_Forest_and_SVM(X_train, X_test, y_train, y_test)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mTfidfvectorizer_customtokeniser_split_data\u001b[39m\u001b[34m(df, ngram)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ngram == \u001b[33m\"\u001b[39m\u001b[33mtri-gram\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     27\u001b[39m     vectorizer = TfidfVectorizer(stop_words = \u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, max_features = \u001b[32m3000\u001b[39m, ngram_range=(\u001b[32m1\u001b[39m,\u001b[32m3\u001b[39m),tokenizer=custom_tokenizer,min_df=\u001b[32m20\u001b[39m,max_df=\u001b[32m0.7\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     X_vector = \u001b[43mvectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     X_train, X_test, y_train, y_test = train_test_split(X_vector, y, test_size=\u001b[32m0.2\u001b[39m, stratify=y, random_state=\u001b[32m26\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m X_train, X_test, y_train, y_test\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2099\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2100\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2101\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2102\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2103\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1368\u001b[39m             warnings.warn(\n\u001b[32m   1369\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m             )\n\u001b[32m   1374\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1379\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1263\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[32m   1262\u001b[39m     feature_counter = {}\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1264\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1265\u001b[39m             feature_idx = vocabulary[feature]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:106\u001b[39m, in \u001b[36m_analyze\u001b[39m\u001b[34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[39m\n\u001b[32m    104\u001b[39m     doc = preprocessor(doc)\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     doc = \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mcustom_tokenizer\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     15\u001b[39m         lemma = token.lemma_.lower().strip()\n\u001b[32m     17\u001b[39m         \u001b[38;5;66;03m# Additional filtering: require at least 3 characters and no special chars\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lemma) >= \u001b[32m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m^[a-zA-Z]+$\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     19\u001b[39m             tokens.append(lemma)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[31mTypeError\u001b[39m: match() missing 1 required positional argument: 'string'"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = Tfidfvectorizer_customtokeniser_split_data(df, \"tri-gram\")\n",
    "Random_Forest_and_SVM(X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
