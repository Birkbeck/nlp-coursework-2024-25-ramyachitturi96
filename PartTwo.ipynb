{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be096ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b579b9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_clean_file(path):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # rename the ‘Labour (Co-op)’ value in ‘party’ column to ‘Labour’\n",
    "    df[\"party\"] = df[\"party\"].replace(\"Labour (Co-op)\", \"Labour\")\n",
    "    #print(df.shape)\n",
    "\n",
    "    # remove any rows where the value of the ‘party’ column is not one of the four most common party names, and remove the ‘Speaker’ value\n",
    "    df = df[df[\"party\"] != \"Speaker\"]\n",
    "    top4_parties = df[\"party\"].value_counts().index[:4]\n",
    "    df = df[df[\"party\"].isin(top4_parties)]\n",
    "    #print(df.shape)\n",
    "\n",
    "    # remove any rows where the value in the ‘speech_class’ column is not ‘Speech’\n",
    "    df = df[df[\"speech_class\"] != \"Speaker\"]\n",
    "    #print(df.shape)\n",
    "\n",
    "    #remove any rows where the text in the ‘speech’ column is less than 1000 characters long.\n",
    "    df = df[df[\"speech\"].str.len() >= 1000]\n",
    "    \n",
    "    print(df.shape)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_and_clean_file(\"p2-texts/hansard40000.csv\")\n",
    "df\n",
    "#df[\"party\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0bac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2a\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b8595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de4c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tfidfvectorizer_split_data(df, ngram: str):\n",
    "    X = df[\"speech\"]\n",
    "    y = df[\"party\"]\n",
    "    \n",
    "    # Below condition will consider unigrams, bi-grams and tri-grams features - d part\n",
    "    # Adjust the parameters of the Tfidfvectorizer so that unigrams, bi-grams and tri-grams will be considered as features\n",
    "    if ngram == \"3-gram\":\n",
    "        vectorizer = TfidfVectorizer(stop_words = \"english\", max_features = 3000, ngram_range=(1,3))\n",
    "\n",
    "    # This condition will be slected for b part\n",
    "    # Use the default parameters, except for omitting English stopwords and setting max_features to 3000.\n",
    "    else :\n",
    "        vectorizer = TfidfVectorizer(stop_words = \"english\", max_features = 3000)\n",
    "    X_vector = vectorizer.fit_transform(X)\n",
    "\n",
    "    # Split the data into a train and test set, using stratified sampling, with a random seed of 26.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_vector, y, test_size=0.2, stratify=y, random_state=26)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0231e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2b\n",
    "X_train, X_test, Y_train, Y_test = Tfidfvectorizer_split_data(df, \"default\")\n",
    "print(\"X Train set shape:\", X_train.shape)\n",
    "print(\"X Test set shape:\", X_test.shape)\n",
    "print(\"Y Train set shape:\", Y_train.shape)\n",
    "print(\"Y Test set shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0db6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e8ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Forest_and_SVM(X_train, X_test, y_train, y_test):\n",
    "    results = {}\n",
    "    #models\n",
    "    random_forest = RandomForestClassifier(n_estimators=300, random_state=26)\n",
    "    svm = SVC(kernel='linear', random_state=26)\n",
    "\n",
    "    #Training models\n",
    "    print(\"Training Random Forest model \\n\")\n",
    "    random_forest.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Training SVM model \\n\")\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    #predictions\n",
    "    random_forest_pred = random_forest.predict(X_test)\n",
    "    svm_pred = svm.predict(X_test)\n",
    "\n",
    "    #macro-average f1 score\n",
    "    random_forest_f1 = f1_score(y_test, random_forest_pred, average='macro')\n",
    "    results[\"Random Forest\"] = random_forest_f1\n",
    "    svm_f1 = f1_score(y_test, svm_pred, average='macro')\n",
    "    results[\"SVM\"] = svm_f1\n",
    "\n",
    "    #classification report \n",
    "    random_forest_report = classification_report(y_test, random_forest_pred)\n",
    "    svm_report = classification_report(y_test, svm_pred)\n",
    "\n",
    "    print(\"Random Forest Model:\")\n",
    "    print(f\"Macro-average f1 score: {random_forest_f1}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(random_forest_report)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"SVM Model:\")\n",
    "    print(f\"Macro-average f1 score: {svm_f1}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(svm_report)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd26466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2c\n",
    "Random_Forest_and_SVM(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32639ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2d\n",
    "X_train, X_test, Y_train, Y_test = Tfidfvectorizer_split_data(df, \"3-gram\")\n",
    "Random_Forest_and_SVM(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a4a60",
   "metadata": {},
   "source": [
    "#part e\n",
    "Implement a new custom tokenizer and pass it to the tokenizer argument of Tfidfvectorizer. You can use this function in any way you like to try to achieve\n",
    "the best classification performance while keeping the number of features to nomore than 3000, and using the same three classifiers as above. Print the classification report for the best performing classifier using your tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d6162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c08d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "\n",
    "    # Process text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        #if not(token.is_space or token.like_num or token.is_stop):\n",
    "             #if token.pos_ in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\",\"\"]:\n",
    "        if token.is_alpha:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "    return tokens\n",
    "\n",
    "def Tfidfvectorizer_customtokeniser_split_data(df, ngram:str):\n",
    "    X = df[\"speech\"]\n",
    "    y = df[\"party\"]\n",
    "\n",
    "    print(\"Starting TfidfVectorizer \\n\")\n",
    "    \n",
    "    if ngram == \"3-gram\":\n",
    "        print(\"Using n-gram range : (1,3) - uni-gram, bi-gram and tri-gram \\n\")\n",
    "        vectorizer = TfidfVectorizer(max_features=3000, ngram_range=(1,3),tokenizer=custom_tokenizer,min_df=20,max_df=0.7)\n",
    "    else:\n",
    "        print(\"Using default n-gram range : (1,1) \\n\")\n",
    "        vectorizer = TfidfVectorizer(max_features=3000, tokenizer=custom_tokenizer, min_df=20, max_df=0.7 )\n",
    "\n",
    "    print(\"Vectorizing data with custom tokenizer \\n\")\n",
    "    X_vector = vectorizer.fit_transform(X)\n",
    "    print(\"Vectorization completed \\n\")\n",
    "\n",
    "    print(\"Splitting the data into training and testing sets \\n\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_vector, y, test_size=0.2, stratify=y, random_state=26)\n",
    "    print(\"Data Split completed \\n\")\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5113fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "start_time = time.time()\n",
    "X_train, X_test, y_train, y_test = Tfidfvectorizer_customtokeniser_split_data(df, \"default\")\n",
    "Random_Forest_and_SVM(X_train, X_test, y_train, y_test)\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Duration for default n-gram: {duration}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"For uni-gram, bi-gram and tri-gram\")\n",
    "X1_train, X1_test, y1_train, y1_test = Tfidfvectorizer_customtokeniser_split_data(df, \"3-gram\")\n",
    "Random_Forest_and_SVM(X1_train, X1_test, y1_train, y1_test)\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Duration for 3-gram: {duration}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
